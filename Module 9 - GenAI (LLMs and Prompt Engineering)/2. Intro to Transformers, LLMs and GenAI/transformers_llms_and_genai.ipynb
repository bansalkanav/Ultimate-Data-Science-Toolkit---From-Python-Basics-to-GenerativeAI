{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfe2a06-0eca-4c7c-9993-68fc60c67061",
   "metadata": {},
   "source": [
    "# **Introduction to Transformers, Large Language Models and GenAI**\n",
    "\n",
    "## **What's Covered**\n",
    "1. What is Language Modeling?\n",
    "2. Auto-Encoding vs Auto-Regression\n",
    "3. What are LLMs?\n",
    "4. Pre-Training, Transfer Learning and Fine-Tuning\n",
    "5. Why Transformers?\n",
    "6. An Era Before Transformers\n",
    "7. Attention is all you need\n",
    "8. A little bit about Transformers\n",
    "9. Advantages of Transformers\n",
    "10. Disadvantages of Transformers\n",
    "11. Popular Modern LLMs\n",
    "    - BERT\n",
    "    - GPT\n",
    "    - T5\n",
    "    - Domain Specific LLMs\n",
    "12. Aligning LLMs With Instructional Prompts\n",
    "13. Prompt Engineering\n",
    "14. Quick Summary about Transformers, LLMs and Prompt Engineering\n",
    "15. What is GenAI?\n",
    "16. Applications of GenAI\n",
    "17. Popular GenAI Approaches\n",
    "18. What Next? How to use LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95a3a9-cd1d-44c9-9ca8-8d3b10ab6fc0",
   "metadata": {},
   "source": [
    "## **What is Language Modeling?**\n",
    "1. If we can model the language, we can solve problems like Machine Translation, Question-Answering, Sentiment Analysis, Conversational Agents much better.\n",
    "2. Language Modeling involves creation of statistical/deep learning models for predicting the likelyhood of a sequence of tokens in a specified vocabulary.\n",
    "3. Two types of Language Modeling Tasks are:  \n",
    "    a. Autoencoding Task  \n",
    "    b. Autoregressive Task  \n",
    "4. **Autoregressive Language Models** are trained to predict the next token in a sentence, based on the previous tokens in the phrase. These models correspond to the **decoder** part of the transformer model. A mask is applied on the full sentence so that the attention head can only see the tokens that came before. These models are ideal for text generatation. For eg: **GPT**\n",
    "5. **Autoencoding Language Models** are trained to reconstruct the original sentence from a corrupted version of the input. These models correspond to the **encoder** part of the transformer model. Full input is passed. No mask is applied. Autoencoding models create a bidirectional representation of the whole sentence. They can be fine-tuned for a variety of tasks, but their main application is sentence classification or token classification. For eg: **BERT**\n",
    "6. **Combination of autoregressive and autoencoding language models** are more versatile and flexible in generating text. It has been shown that the combination models can generate more diverse and creative text in different context compared to pure decode-based autoregressive models due to their ability to capture additional context using the encoder. For eg: **T5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f083fa-036e-42a4-b46c-f5f6e8ecbd94",
   "metadata": {},
   "source": [
    "## **AutoEncoding Vs AutoRegressive Task**\n",
    "Autoencoding and autoregressive tasks are both types of sequence generation tasks in the field of machine learning, but they have distinct differences in their objectives and approaches:\n",
    "\n",
    "**Autoencoding Task**:\n",
    "1. **Objective**:\n",
    "   - In an autoencoding task, the model is trained to reconstruct the input sequence from a corrupted or noisy version of itself.\n",
    "   - The objective is to learn a representation of the input data that captures its essential features while filtering out noise or irrelevant information.\n",
    "2. **Bidirectional Learning**:\n",
    "   - Autoencoding models are bidirectional, meaning they learn to generate sequences by considering both past and future context simultaneously.\n",
    "   - The model encodes the entire input sequence into a fixed-size representation (encoding) and then decodes it back into the original sequence (decoding).\n",
    "3. **Training Signal**:\n",
    "   - The training signal for an autoencoding task comes from comparing the reconstructed output with the original input.\n",
    "   - The model adjusts its parameters to minimize the discrepancy between the input and reconstructed output, typically using a reconstruction loss such as mean squared error (MSE) or binary cross-entropy.\n",
    "\n",
    "**Autoregressive Task**:\n",
    "1. **Objective**:\n",
    "   - In an autoregressive task, the model is trained to generate the next token in a sequence given the preceding tokens.\n",
    "   - The objective is to model the conditional probability distribution of each token in the sequence given its predecessors.\n",
    "2. **Unidirectional Learning**:\n",
    "   - Autoregressive models are unidirectional, meaning they generate sequences one token at a time in a left-to-right fashion.\n",
    "   - At each time step, the model predicts the next token based only on the tokens generated so far, without considering future context.\n",
    "3. **Training Signal**:\n",
    "   - The training signal for an autoregressive task comes from comparing the model's predictions for each token with the ground truth.\n",
    "   - The model adjusts its parameters to maximize the likelihood of generating the correct tokens in the sequence, typically using a cross-entropy loss.\n",
    "\n",
    "**Key Differences**:\n",
    "1. **Learning Approach**:\n",
    "   - Autoencoding models learn to reconstruct the input sequence, while autoregressive models learn to generate new sequences one token at a time.\n",
    "2. **Directionality**:\n",
    "   - Autoencoding models are bidirectional, considering both past and future context, while autoregressive models are unidirectional, considering only past context.\n",
    "3. **Training Signal**:\n",
    "   - Autoencoding models minimize reconstruction error between input and output sequences, while autoregressive models maximize the likelihood of generating the correct tokens in the sequence.\n",
    "\n",
    "Overall, autoencoding and autoregressive tasks have different objectives and learning approaches, but both are used for sequence generation tasks in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904d47f-152a-48c9-9911-5e28a2add988",
   "metadata": {},
   "source": [
    "## **What are LLMs?**\n",
    "1. Usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "2. Designed to understand and generate human language, code, and much more.\n",
    "3. Highly parallelized and scalable.\n",
    "4. Example: BERT, GPT and T5\n",
    "5. Techniques like: Stop word removal, stemming, and truncation are not used nor are they necessary for LLMs. LLMs are designed to handle the inherent complexity and variability of human language, including the use of stop words and variations in word forms like tenses and misspellings.\n",
    "6. Every LLM on the market has been **pre-trained** on a large corpus of the text data and on a specific language modeling related tasks.\n",
    "7. **Remember:** How an LLM is **pre-trained** and **fine-tuned** makes all the difference.\n",
    "8. **How to decide whether to train our own embeddings or use pre-trained embeddings?** - A good rule of thumb is to compute the vocabulary overlap. If the overlap between the vocabulary of our custom domain and that of pre-trained word embeddings is significant, pre-trained word embeddings tends to give good results.\n",
    "9. **One more important factor to consider while deploying models with embeddings-based feature extraction approach:** - Remember that learned or pre-trained embedding models have to be stored and loaded into memory while using these approaches. If the model itself is bulky, we need to factor this into our deployment needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4674a79-a1d2-4613-b633-f5cdc5dd31f8",
   "metadata": {},
   "source": [
    "## **Pre-Training, Transfer Learning and Fine-Tuning**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/transfer_learning.jpeg\">\n",
    "\n",
    "1. **Pre-training** of an LLM happens on a large corpus of text data and on a specific language modeling related task. During this phase LLM tries to learn and understand general language and relationships between words.\n",
    "2. **Transfer Learning** is a technique used in machine learning to leverage the knowledge gained from one task to improve performance on another related task. Understand that pre-trained model has already learned a lot of information about the language and the relationships between words, and this information can be used as a starting point to improve performance on a new task.  \n",
    "    **a.** Transfer Learning for LLMs involves taking an LLM that has been pre-trained on one corpus of text data and then fine-tuning it for a specific downstream task, such as text classification or text generation, by updating the model's parameter with task-specific data.  \n",
    "    **b.** Transfer Learning allows LLMs to be **fine-tuned** for specific tasks with much smaller amounts of task-specific data than it would require if the model were trained from scratch. This greatly reduces the amount of time and resources required to train LLMs.  \n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/fine_tuning_loop.jpeg\">\n",
    "3. **Fine-tuning** involves training the LLM on a smaller, task-specific dataset to adjust its parameters for the specific task at hand. The basic fine-tuning loop is more or less same.  \n",
    "    **a.** Define a model you want to fine-tune as well as fine-tuning parameters (eg: learning rate)  \n",
    "    **b.** Aggregate some training data.  \n",
    "    **c.** Compute loss and gradients.  \n",
    "    **d.** Update the model via backpropogation.  \n",
    "4. The Transformers package from Hugging Face provides a neat and clean interface for training and fine-tuning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013b87c-621a-4757-8024-5deb33a46b61",
   "metadata": {},
   "source": [
    "## **Why Transformers?**\n",
    "\n",
    "1. Scalable and Parallel Compute\n",
    "2. Revolutionized NLP with LLMs\n",
    "3. Unification of DL Approaches\n",
    "4. Multi-Modal Capability\n",
    "5. Accelerated GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c515617-57f8-42f5-b1b1-13406bb8e8e3",
   "metadata": {},
   "source": [
    "## **An Era Before Transformers**\n",
    "\n",
    "1. **2013 and before:** Various Neural Network Architectures like ANN, CNN and RNN became very popular. They use to work well for tabular data, image data and sequential data like text respectively.\n",
    "2. **[(2014) Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)** paper introduced the concept of **Encoder-Decoder Architecture** to solve a seq2seq task, like machine translation.\n",
    "    - The paper introduces Seq2Seq models, which are neural network architectures designed for mapping input sequences to output sequences. Unlike traditional models that rely on fixed-length input-output mappings, Seq2Seq models can handle variable-length sequences, making them suitable for tasks such as machine translation, summarization, and question answering.\n",
    "    - The core of the Seq2Seq model is the encoder-decoder architecture. The encoder processes the input sequence while maintaining the hidden state and generates a fixed-length representation, often referred to as a context vector. This context vector encapsulates the representation of the whole sentence.\n",
    "    - The decoder then uses this representation to generate the output sequence one token at a time.\n",
    "    - Both encoder and decoder used RNN/LSTM cells due to their ability to capture sequential dependencies.\n",
    "    - This architecture used to work well with smaller sentence.\n",
    "    - **The Problem:** While it could handle variable-length input and output sequences, it used to rely on generating a single fixed-length context vector for the entire input sequence, which can lead to information loss, especially for longer sequences.\n",
    "3. **[(2015) Neural Machine Translation by Joint Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)** paper introduced the concept of **Attention Mechanism** to solve the above problem.\n",
    "    - Unlike traditional NMT models that encode the entire source sentence into a fixed-length context vector, the **attention mechanism allows the model to focus on different parts of the source sentence dynamically** while generating the translation.\n",
    "    - Attention Mechanism also **addressed the problem of learning alignment between input and output sequences**, enables the model to weigh the importance of each word in the source sentence differently during translation. By dynamically adjusting the attention weights, the model can focus more on relevant words and ignore irrelevant ones, leading to more accurate translations. For eg: Think about the english to hindi translation for \"I work at Apple Inc\" vs \"I work at Apple Farm\". Where should I keep ‡§∏‡•á‡§¨ vs ‡§è‡§™‡•ç‡§™‡§≤ ‡§á‡§Ç‡§ï ?\n",
    "    - At each timestamp of the decoder, the dynamically calculated context vector indicates which timestamps of the encoder sequence are expected to have the most influence on the current decoding step of the decoder.\n",
    "    - In simple terms, context vector will be the weighted sum of encoders hidden state. And these weights are called as **attention weights**.\n",
    "    - The attention mechanism has improved, the quality of translation on long input sentences. But it was not able to solve a huge fundamental flaw i.e. sequential training.\n",
    "    - **The Problem:** Since the architecture relies on LSTM units, a notable challenge arises due to the sequential nature of training. Specifically, only one token can be processed at a time as input to the encoder, leading to slow training times. Consequently, it becomes impractical to train the model efficiently with large datasets. This limitation inhibits the application of techniques like transfer learning, which typically involve leveraging pretrained models on large datasets to improve performance on new tasks. Additionally, fine-tuning, which involves further training pretrained models on task-specific data, is also hindered by the slow training process in this architecture.\n",
    "    - Now because of the above problem, for any task which we are suppose to solve, we have to train the model from scratch. And it takes a huge amount of time, efforts and data.\n",
    "    - **Transfer Learning:** Transfer learning involves leveraging knowledge gained from solving one problem and applying it to a different, but related, problem.\n",
    "    - **Fine-Tuning:** Fine-tuning, on the other hand, refers to the process of taking a pretrained model and further training it on task-specific data to adapt it to a particular problem or domain. This typically involves adjusting the parameters of the pretrained model to better suit the new task while retaining the knowledge learned from the original training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00ce99-138d-4b96-ab80-f24928eb4029",
   "metadata": {},
   "source": [
    "## **Attention is all you need: Introducing Transformer Architecture**\n",
    "\n",
    "<img style=\"float: right;\" width=\"400\" height=\"600\" src=\"data/images/transformer.JPG\">\n",
    "\n",
    "**[(2017) Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)** paper introduced by Google which solves the sequential training problem of earlier architecture by removing the need of RNN cells completely.\n",
    "1. Transformer has the encoder-decoder architecture.\n",
    "2. **Encoder** is great at understanding text.\n",
    "3. **Decoder** is great at generating text.\n",
    "4. Transformer relies solely on self-attention mechanisms and feed-forward neural networks.\n",
    "5. Understand that **Attention** is a mechanism that assigns different weights to different parts of the input allowing the model to prioritize and emphasize the most important information while performing tasks like translation or summarization. Attention allows a model to focus on different parts of the input dynamically, leading to improved performance.\n",
    "6. **Positional Encoding:** To retain positional information of words in the input sequence without using recurrence, the model introduces positional encodings. These encodings are added to the input embeddings to provide information about the position of each word in the sequence.\n",
    "7. **Self-Attention Mechanism:** The key innovation of the Transformer is the self-attention mechanism, which allows each word in the input sequence to attend to all other words in the sequence. This enables capturing global dependencies and alleviates the need for recurrent connections.\n",
    "8. **Multi-Head Attention:** The Transformer employs multi-head attention mechanisms, where attention is computed multiple times in parallel with different learned linear projections. This allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture diverse patterns.\n",
    "9. **Feed-Forward Network:** The main goal of the feed-forward network is to apply non-linear transformations to the input representations, helping the model capture complex patterns and relationships in the input sequences. This helps enriching the representations of words or tokens in the input sequence.\n",
    "10. **Skip/Residual Connections:** The main goal of skip connections is to enable the network to retain important information from previous layers and make it easier for the model to learn and optimize complex patterns in the data. Think of skip connections as shortcuts that allow information to bypass certain layers in the network. These shortcuts ensure that important information from earlier layers is preserved and remains accessible to later layers.\n",
    "11. **Parallelization and Scalability:** By relying on self-attention mechanisms and feed-forward layers, the Transformer architecture facilitates parallelization of computation across different parts of the input sequence. This results in faster training times and better scalability compared to traditional recurrent models.\n",
    "\n",
    "**Below you can find an image of Full Attentions for head 5 (from transformer original paper). The image shows the relationships learned between words with the help of self-attention mechanism.**\n",
    "<img width=\"800\" height=\"300\" src=\"data/images/attention_mechanism_full.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b6f3b-e318-4607-baea-878ecd6fef78",
   "metadata": {},
   "source": [
    "## **A little bit about Transformers**\n",
    "\n",
    "<img style=\"float: right;\" width=\"300\" height=\"500\" src=\"data/images/attention_mechanism_isolated_for_word_its.JPG\">\n",
    "\n",
    "1. Introduced by Google in the year 2017\n",
    "2. Transformer is a Sequence to Sequence Model which was proposed initially to solve the task of Machine Translation\n",
    "3. Has two main components: Encoder-Decoder and Attention Mechanism\n",
    "4. An **encoder** which is tasked with taking in raw text, splitting them up into its core components, convert them into vectors and using **self-attention** to understand the context of the text.\n",
    "5. A **decoder** excels at generating text by using a modified type of attention (i.e. **cross attention**) to predict the next best token.\n",
    "6. Transformers revolutionized NLP by enabling highly scalable training. By leveraging parallel computation and efficient self-attention mechanisms, the Transformer architecture allows for training on massive datasets with unprecedented efficiency. This scalability laid the foundation for the concept of **Transfer Learning** in NLP. Subsequent models such as BERT, GPT, and T5 were developed, leveraging pre-trained Transformer-based architectures that could be easily **fine-tuned** for a wide range of NLP tasks, further advancing the field of natural language processing.\n",
    "7. Transformers are **trained** to solve a specific NLP task called as **Language Modeling**.\n",
    "8. **Why not RNNs? -** RNN units can become a bottleneck due to sequential training. Due to parallel training capabilities and self attention mechanism of transformer, it allows each word to \"attend to\" all the other words in the sequence which enables it to capture long-term dependencies and contextual relationships between words at scale. The goal is to understand each word as it relates to the other tokens in the input text.\n",
    "9. **Limitations of Transformers:** Transformers are still limited to an input context window (i.e. maximum length of text it can process at any given moment)\n",
    "10. Timeline\n",
    "    - Till 2013 - RNN/LSTMs/GRU\n",
    "    - 2014 - Seq2seq tasks using Encoder-Decoder architecture\n",
    "    - 2015 - Attention Mechanism\n",
    "    - 2017 - Transformers\n",
    "    - 2018 - BERT by Google / GPT by OpenAI\n",
    "    - 2019 - T5 by Google\n",
    "    - 2020 - Stable Diffusion / GPT3\n",
    "    - 2021 - DALL-E / Github Copilot\n",
    "    - 2022 - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e00c8e-37b5-4f8a-9122-d948b9dd5bee",
   "metadata": {},
   "source": [
    "## **Advantages of Transformers**\n",
    "1. Parallel Training and Scalable\n",
    "2. Transfer Learning\n",
    "3. Multimodal Input and Output\n",
    "4. Flexible Architecture: Encoder only transformer models like BERT, Decoder only transformer like GPT and Encode-Decoder based model like T5.\n",
    "5. Ecosystem: HuggingFace, OpenAI, Cohere, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b0961-1a21-437f-914c-5ae1e5991638",
   "metadata": {},
   "source": [
    "## **Disadvantages of Transformers**\n",
    "1. Needs high computational resources like space and GPUs\n",
    "2. Huge amount of Data is required to train a model using transformers\n",
    "3. Overfitting\n",
    "4. Energy/Electricity Consumptions\n",
    "5. Interpretation\n",
    "6. Biasness due to data and Ethical Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588172b-9c70-4c0f-9732-c5d00b22967f",
   "metadata": {},
   "source": [
    "## **Popular Modern LLMs**\n",
    "\n",
    "### **1. BERT (Bidirectional Encoder Representation from Transformers)**\n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_oov.jpeg\">\n",
    "\n",
    "1. By Google - Autoencoding Language Model\n",
    "2. **[Click Here](https://arxiv.org/pdf/1810.04805.pdf)** to read the original paper from Google - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\n",
    "3. Individual NLP tasks have traditionally been solved by individual models created for each specific task. That is, until‚Äî BERT!\n",
    "4. Tasks - BERT can solve 11+ NLP tasks such as sentiment analysis, named entity recognition, question answering, etc...\n",
    "5. **Data:** Pretrained on:  \n",
    "    **a.** English Wikipedia - At the time 2.5 Billion words  \n",
    "    **b.** Book Corpus - 800 Million words  \n",
    "6. Training on a dataset this large takes a long time. BERT‚Äôs training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Google‚Äôs custom circuit built specifically for large ML models). ~64 TPUs trained BERT over the course of 4 days.\n",
    "7. **Input to BERT:** BERT uses three layer of token embedding for a given piece of text: Token Embedding, Segment Embedding (To distinguish between segment A and B) and Position Embedding. \n",
    "    - BERT uses WordPiece Embeddings with a 30,000 token vocabulary. The first token of every sequence is always a special classification token i.e. `[CLS]`.\n",
    "    - Sentence Pairs are packed together into a single sequence with a special separator i.e. `[SEP]`.\n",
    "    - For a given token, its input representation is constructed by summing the corresponding token, segment and position embedding.\n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_language_model_task.jpeg\">\n",
    "8. **Out-of-vocabulary words with BERT** BERT's tokenizer handles OOV tokens (out of vocabulary / previously unknown) by breaking them up into smaller chunks of known tokens.\n",
    "9. **BERT Training:** Trained on two language modeling specific tasks:  \n",
    "    **a.** **Masked Language Modeling (MLM) aka Autoencoding Task** - (Covered in detail below) Helps BERT recognize token interaction within the sentence.    \n",
    "    **b.** **Next Sentence Prediction (NSP) Task** - This task helps BERT learn relationships between sentences and helps it to understand how tokens interact with each other between sentences. It is acheived by predicting whether Sentence B is actual sentence that proceeds Sentence A, or a random sentence. \n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_classification.jpeg\">\n",
    "10. **Why is BERT Training Fast?** BERT uses the encoder of transformer and ignores the decoder to become exceedingly good at processing/understanding massive amounts of text very quickly relative to other slower LLMs that focus on generating text one token at a time.\n",
    "11. **BERT-Base:** 12 transformer layers, 768 hidden size, 12 attention heads, 110M parameters (This was trained on 4 cloud TPUs for 4 days)\n",
    "12. **BERT-Large:** 24 transformer layers, 1024 hidden size, 16 attention heads, 340M parameters (This was trained on 16 cloud TPUs for 4 days)\n",
    "13. BERT itself doesn't classify text or summarize documents but it is often used as a pre-trained model for downstream NLP tasks. \n",
    "14. 1 year later RoBERTa by Facebook AI shown to not require NSP task. It matched and even beat the original BERT model's performance in many areas. Other models:\n",
    "    - RoBERTa: A Robustly Optimized BERT Pretraining Approach. Trained BERT for more epochs and/or on more data. Used improved masking and pre-training data slightly.\n",
    "    - ALBERT: A Lite BERT. Use smaller embedding size. It is lite in terms of parameters, not speed.\n",
    "    - T5: Text-To-Text Transfer Tranformer. Has 11B parameters. Trained on 120B words of cleaned common crawl text\n",
    "15. Reference: [Click here to read more](https://huggingface.co/blog/bert-101)\n",
    "16. BERT Implementation: [Click here to learn how to use BERT](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)\n",
    "\n",
    "#### **Auto-Encoding Task vs Masked Language Modeling (MLM)**   \n",
    "- **Problem with Auto-Encoding:** As it is \"bidirectional\" input reconstruction task without masking, words can see themselves.\n",
    "- Solution: **Masked LM (proposed by BERT)**\n",
    "- MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word. We naturally do this as humans!\n",
    "- For eg: \"How are `[MASK]` doing today?\"\n",
    "- Can you guess the `[MASK]`. You‚Äôre naturally able to predict the missing word by considering the words bidirectionally before and after the missing word as context clues. \n",
    "- Here it can be 'you', 'they', etc... Prediction will 'you' as it has highest probability here.\n",
    "- **Fun Fact:** Newer models like BERT can be more accurate than humans! ü§Ø\n",
    "- **What is Masked LM?** The bidirectional methodology you did to fill in the `[MASK]` word above is similar to how BERT attains state-of-the-art accuracy. **It only predict the masked words rather than reconstructing the entire input.**\n",
    "- In BERT, we mask out a random k% of the input words, and then BERT's job is to correctly predict the masked words. We always use k = 15%.\n",
    "- Too little masking: Too expensive to train.\n",
    "- Too much masking: Not enough context to learn.\n",
    "- **Problem with Masked LM:**\n",
    "    - There's a problem when using BERT for fine-tuning on specific tasks. During pre-training, BERT uses a special token `[MASK]` to mask certain words in the input sentence and trains the model to predict these masked words. But during fine-tuning (when using BERT for specific tasks like classification or translation), the `[MASK]` token does not exist in the input data.\n",
    "    - To address this mismatch, the authors describe a strategy where they don't always replace words with the `[MASK]` token during pre-training.\n",
    "    - k=15% of the words to mask, but don't replace with `[MASK]` 100% of the time. Instead: only 80% of the time replace with `[MASK]`. 10% of the time replace with a random word. 10% of the time keep the same word.\n",
    "    - With 10% random word replacement, BERT learns to handle noisy input and become more robust to variations in the training data.\n",
    "\n",
    "Auto-encoding and Masked LM are both pre-training tasks used in NLP to train transformer-based models like BERT. While they are very similar, but they have distinct differences. Both tasks aim to learn rich, context-aware representations of words and sentences, but they achieve this goal through different training objectives and mechanisms:\n",
    "1. Objective:\n",
    "    - In an autoencoding task, the model is trained to reconstruct the original input sequence from a corrupted or noisy version of the same sequence.\n",
    "    - In a masked language model task, the model is trained to predict masked or missing words in an input sequence.\n",
    "2. Input-Output Relationship\n",
    "    - In autoencoding task, the model learns to map the input sequence to itself, with the goal of minimizing the reconstruction error between the input and output sequences.\n",
    "    - In MLM, the model's task is to predict the original words that were replaced with `[MASK]` tokens based on the surrounding context.\n",
    "3. Training\n",
    "    - In autoencoding task, the model adjusts its parameters to minimize the discrepancy between the input and reconstructed output, typically using a reconstruction loss such as mean squared error (MSE) or binary cross-entropy.\n",
    "    - The model adjusts its parameters to minimize the discrepancy between the predicted tokens and the original tokens, typically using a cross-entropy loss.\n",
    "\n",
    "\n",
    "### **2. GPT (Generative Pre-Trained Transformer)**\n",
    "\n",
    "1. By OpenAI - Autoregressive Language Model\n",
    "2. **[Click Here](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)** to read the original paper from OpenAI - Improving Language Understanding by Generative Pre-Training.\n",
    "3. Pretrained on: Proprietary Data (Data for which the rights of ownership are restricted so that the ability to freely distribute the is limited)\n",
    "4. Autoregressive Language Model that uses attention to predict the next token in a sequence based on the previous tokens.\n",
    "5. GPT relies on the decoder portion of the Transformer and ignores the encoder to become exceptionally good at generating text one token at a time.\n",
    "\n",
    "\n",
    "### **3. T5 (Text to Text Transfer Transformer)**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/t5.jpeg\">\n",
    "\n",
    "1. In 2019, By Google - Combination of Autoencoder and Autoregressor Language Model.\n",
    "2. Has 11B parameters. Trained on 120B words of cleaned common crawl text\n",
    "3. Tasks: T5 can solve tasks such as summarization, translation, Q&A, and text classification\n",
    "4. T5 uses both encoder and decoder of the Transformer to become highly versatile in both processing and generating text.\n",
    "5. T5 based models can generate wide range of NLP tasks, from text classification to generation.\n",
    "\n",
    "\n",
    "### **4. Domain Specific LLMs**\n",
    "\n",
    "1. BioGPT - Trained on large scale biomedical literature (more than 2 million articles). Developed by the AI healthcare company, Owkin, in collaboration with Hugging Face.\n",
    "2. SciBERT\n",
    "3. BlueBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7883b6-b105-48b0-ad65-fc133542d942",
   "metadata": {},
   "source": [
    "## **Aligning LLMs With Instructional Prompts**\n",
    "1. After the initial pre-training, the LLM may undergo fine-tuning where it is further trained on specific tasks or domains. During fine-tuning, the model is exposed to additional data related to the task or domain, along with instructional prompts tailored to the task. This helps the model adapt and specialize for specific applications or use cases.\n",
    "2. A popular method of aligning language model is through the incorporation of **Reinforcement Learning** into the training loop.\n",
    "3. **Reinforcement Learning with Human Feedback (RLHF)** is a popular method of aligning pre-trained LLMs that uses human feedback to enhance their performance.\n",
    "4. Few Language Models that have been specifically designed and trained to be aligned with instructional prompts are GPT-3, GPT-4, ChatGPT (closed-source model from OpenAI), FLAN-T5 (an open-source model from Google) and Cohere's command series (closed-source)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65c3cf-2acf-42d1-9307-17b35f38ea07",
   "metadata": {},
   "source": [
    "## **Prompt Engineering**\n",
    "1. Popular LLMs like GPT-3, GPT-4, ChatGPT, Coral, GPT-J, FLAN-T5, etc... have been specifically designed and **trained to be aligned with instructional prompts**.\n",
    "2. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**.\n",
    "3. **Prompt Engineering** involves crafting prompts that effectively communicate the task at hand to the LLM, leading to accurate and useful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded9146-c262-428f-9042-23f463695a03",
   "metadata": {},
   "source": [
    "## **Quick Summary about Transformers, LLMs and Prompt Engineering**\n",
    "1. What really sets the Transformers appart from other deep learning architectures is:\n",
    "    - Its ability to capture long-term dependencies and relationships between tokens using attention mechanism.\n",
    "    - Its ability to scale and parallelize the computation\n",
    "2. Attention is the crucial component of Transformer.\n",
    "3. Factor behind transformer's effectiveness as a language model is it is highly parallelizable, allowing for faster training and efficient processing of text.\n",
    "4. LLMs are usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "5. Designed to understand and generate human language, code, and much more.\n",
    "6. LLMs are pre-trained on large corpus and fine-tuned on smaller datasets for specific tasks.\n",
    "7. Few Popular LLMs: BERT, GPT-4, GPT-3.5, Gemini (Previously known as Bard), Cohere, LLaMa-2, Coral, GPT-J, FLAN-T5, etc...\n",
    "8. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**.\n",
    "\n",
    "Remember, building an LLM requires a huge amount of good quality data and computational resources. **[Read here](https://blog.google/products/gemini/gemini-image-generation-issue/)** where google explained what went wrong after the launch of Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853b13b-647e-469b-8039-8f062f791ee5",
   "metadata": {},
   "source": [
    "## **What is Generative AI?**\n",
    "GenAI System typically learns the patterns from unstructured input data and learns to generate unstructured output data.\n",
    "Remember that, when the output of a model is one of the following, it is an example of GenAI model:\n",
    "1. Text\n",
    "2. Image\n",
    "3. Video\n",
    "4. Audio\n",
    "\n",
    "## **Applications of GenAI**\n",
    "1. Text Generation\n",
    "2. Summarization\n",
    "3. Code Generation\n",
    "4. Machine Translation\n",
    "5. Virtual Assistants\n",
    "6. Question Answering\n",
    "7. Image Editing\n",
    "8. Image Generation\n",
    "9. Image Inpainting\n",
    "10. etc...\n",
    "\n",
    "## **Popular GenAI Approaches**\n",
    "1. **Autoencoding**\n",
    "    - In autoencoding, the model is trained to reconstruct the input data. It consists of two main components: an encoder and a decoder.\n",
    "    - The encoder takes the input data and maps it to a latent space, where it is represented in a compressed form.\n",
    "    - The decoder then takes this compressed representation and tries to reconstruct the original input data from it.\n",
    "    - The goal of autoencoding is to learn a compact representation of the input data that captures its salient features, allowing the model to generate new samples similar to the training data.\n",
    "2. **Generative Adversarial Networks**\n",
    "    - GANs consist of two neural networks: a generator and a discriminator, which are trained adversarially against each other.\n",
    "    - The generator learns to generate realistic data samples from random noise.\n",
    "    - The discriminator learns to distinguish between real data samples from the training set and fake data samples generated by the generator.\n",
    "    - During training, the generator tries to generate data samples that are indistinguishable from real samples, while the discriminator tries to correctly classify real and fake samples.\n",
    "    - The objective of GANs is to learn to generate new data samples that are realistic and similar to the training data, without explicitly reconstructing the input data like autoencoders.\n",
    "    - While both autoencoding and GANs are used for generative modeling, autoencoding focuses on reconstructing the input data, while GANs focus on generating new data samples from random noise.\n",
    "3. **Autoregressive**\n",
    "    - Autoregressive models generate new data samples by modeling the conditional probability of each data point given previous data points in the sequence.\n",
    "    - Autoregressive models are often used for sequential data, such as time series data, where the order of elements is important.\n",
    "    - Unlike autoencoders, which learn to reconstruct the input data, and GANs, which learn to generate samples from random noise, autoregressive models explicitly model the sequential dependencies in the data and generate new samples one element at a time based on these dependencies.\n",
    "    - Autoregressive models differ from autoencoding and GANs in that they explicitly model the sequential dependencies in the data and generate new samples one element at a time based on these dependencies, rather than focusing on reconstructing the input data or generating samples from random noise.\n",
    "4. **Stable Diffussion**\n",
    "    - Stable diffusion models generate new data samples by iteratively refining a noise input through multiple steps.\n",
    "    - These models gradually add noise to the input noise and refine it through a series of diffusion steps, effectively diffusing the noise until it resembles the target distribution of the data.\n",
    "    - Stable diffusion models often use deep neural networks, such as convolutional neural networks (CNNs) or transformer architectures, to perform the diffusion process and generate high-quality samples.\n",
    "    - The objective of stable diffusion models is to learn the underlying distribution of the data and generate new samples that are realistic and similar to the training data, without relying on explicit modeling of sequential dependencies or adversarial training.\n",
    "    - Stable diffusion models are particularly effective for generating high-resolution images and other complex data types where capturing fine-grained details and global coherence is important.\n",
    "    - Stable diffusion models differ from autoencoding, GANs, and autoregressive models in that they generate new data samples by iteratively refining a noise input through multiple steps, rather than focusing on reconstructing the input data, generating samples from random noise, or explicitly modeling sequential dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95ec3b-ea35-47e2-92e9-a8c845c05b1e",
   "metadata": {},
   "source": [
    "## **What Next? How to use LLMs?**\n",
    "\n",
    "Given a business problem, ask this to yourself:\n",
    "1. What NLP task does it map to?\n",
    "    - Text Classification\n",
    "    - Token Classification\n",
    "    - Text Generation\n",
    "    - Fill-Mask\n",
    "    - Conversational\n",
    "    - Sentence Similarity\n",
    "    - Question Answer\n",
    "    - Summarization\n",
    "    - Table Q&A\n",
    "    - Translation\n",
    "    - Zero-Shot Classification\n",
    "2. Given the task, what model(s) work for that task?\n",
    "\n",
    "**Example:**  \n",
    "> **Business Problem:** Generate a news feed for an app so that users can scroll through  \n",
    "> **Mapping to a NLP task:** Given news article, a standard NLP task is to summarize  \n",
    "\n",
    "Now before we get into how to solve problems like above, a quick note on NLP ecosystem:\n",
    "\n",
    "| Popular Tools | Utility |\n",
    "| :---: | :---: |\n",
    "| **Hugging Face Transformers** | Pre-trained models and Pipelines |\n",
    "| **NLTK** | Classical NLP + corpora |\n",
    "| **SpaCy** | Production grade NLP, especially NER |\n",
    "| **Gensim** | Classical NLP + Word2Vec |\n",
    "| **OpenAI** | ChatGPT, Whisper |\n",
    "| **Spark NLP** | Scale-out, production-grade NLP |\n",
    "| **LangChain** | LLM Workflows |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
