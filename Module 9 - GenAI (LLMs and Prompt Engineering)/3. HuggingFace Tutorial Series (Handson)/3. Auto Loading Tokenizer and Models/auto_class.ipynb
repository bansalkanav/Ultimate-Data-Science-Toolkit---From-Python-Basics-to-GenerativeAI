{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32fcd43-ea2e-49b4-b3a7-50cd45372ff9",
   "metadata": {},
   "source": [
    "# **Auto Class**  \n",
    "\n",
    "We have already seen the `dataset` and `pipeline` abstractions from Hugging Face.  While a `pipeline` is a quick way to set up an LLM for a given task, the slightly lower-level abstractions `model` and `tokenizer` permit a bit more control over options.  \n",
    "\n",
    "<img width=\"800\" height=\"500\" src=\"data/images/hugging_face_transformers_pipeline.jpeg\">\n",
    "\n",
    "## **Pipeline**\n",
    "1. Preprocessing Data (i.e. Tokenize the input)\n",
    "2. Use the Model to solve the Task (i.e. Pass the input to appropriate Model)\n",
    "\n",
    "#### **Preprocessing Data**\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you‚Äôll learn that for:\n",
    "- Text, use a `AutoTokenizer` to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.\n",
    "- Speech and audio, use a `AutoFeatureExtractor` to extract sequential features from audio waveforms and convert them into tensors.\n",
    "- Image inputs use a `AutoImageProcessor` to convert images into tensors.\n",
    "- Multimodal inputs, use a `AutoProcessor` to combine a tokenizer and a feature extractor or image processor.\n",
    "\n",
    "**Note: `AutoProcessor` always works and automatically chooses the correct class for the model you‚Äôre using, whether you‚Äôre using a tokenizer, image processor, feature extractor or processor.**\n",
    "\n",
    "\n",
    "#### **Solving the Task**\n",
    "After the data has been preprocessed and converted to vectors, we can use the following pre-trained Auto classes for solving [Natural Language Processing](https://huggingface.co/docs/transformers/model_doc/auto#natural-language-processing), [Computer Vision](https://huggingface.co/docs/transformers/model_doc/auto#computer-vision), [Audio](https://huggingface.co/docs/transformers/model_doc/auto#audio) and [Multimodal](https://huggingface.co/docs/transformers/model_doc/auto#multimodal) Tasks :\n",
    "1. AutoModelFor\n",
    "2. TFAutoModelFor\n",
    "3. FlaxAutoModelFor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References:   \n",
    "    - https://huggingface.co/docs/transformers/model_doc/auto  \n",
    "    - https://huggingface.co/docs/transformers/autoclass_tutorial\n",
    "\n",
    "We will first look at the [Auto* classes](https://huggingface.co/docs/transformers/model_doc/auto) for tokenizers and model types which can simplify loading pre-trained tokenizers and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59deb69d-2f39-463e-86e7-dadce9969f78",
   "metadata": {},
   "source": [
    "## **Building the Summarization Pipeline**\n",
    "\n",
    "Recall the `xsum` dataset from the **Summarization** section before.\n",
    "\n",
    "**Steps**\n",
    "1. Load the data\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('xsum')\n",
    "```\n",
    "2. Define the pipeline by specifying the task and model\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "                task=\"summarization\",\n",
    "                model=\"t5-small\"\n",
    ")\n",
    "```\n",
    "3. Use `summarizer` to summarize the articles\n",
    "```python\n",
    "summarizer(article)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32f7ff20-04ac-41ac-a49f-401239a8629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow\n",
    "# ! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7401df-a6be-4a4d-9ba5-a313fc897425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "xsum_dataset = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213decd8-db25-46e9-ad9d-8e6c55bebe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_sample = xsum_dataset['train'].select(range(10))\n",
    "\n",
    "xsum_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029cc98f-42bb-4ced-b2aa-a1ef33eea09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Define the pipeline by specifying the task and model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "                task=\"summarization\",\n",
    "                model=\"t5-small\",\n",
    "                truncation=True\n",
    ")\n",
    "\n",
    "# If we donot set truncation=True, we get the following warning during inference:\n",
    "# Token indices sequence length is longer than the specified maximum \n",
    "# sequence length for this model (541 > 512). Running this sequence \n",
    "# through the model will result in indexing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5acd79a-c6a5-435f-92e1-4d45e558e49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the water breached a retaining wall, flooding many commercial properties .'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3 - Use summarizer to summarize the articles\n",
    "\n",
    "summarizer(xsum_sample[\"document\"][0], do_sample=True, top_k=10, top_p=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032a91-234d-41b8-86ed-ac58896f6b85",
   "metadata": {},
   "source": [
    "## **AutoTokenizer**\n",
    "\n",
    "A tokenizer takes text as input and outputs numbers the associated model can make sense of.\n",
    "\n",
    "<img width=\"400\" height=\"400\" src=\"data/images/tokenization.JPG\">\n",
    "\n",
    "Let's learn the step by step process now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd6c504-cb8d-4280-9ea2-02fb75ef1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']\n",
      "\n",
      "Tokens Id: [2292, 1005, 1055, 3046, 2000, 19204, 4697, 999]\n",
      "\n",
      "Tokens Id with special tokens: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n",
      "\n",
      "Decoded Text Output: [CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8334035-bab7-4ec7-9fe7-feaf79c97805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['Let', \"'s\", 'ƒ†try', 'ƒ†to', 'ƒ†token', 'ize', '!']\n",
      "\n",
      "Tokens Id: [7939, 18, 860, 7, 19233, 2072, 328]\n",
      "\n",
      "Tokens Id with special tokens: [0, 7939, 18, 860, 7, 19233, 2072, 328, 2]\n",
      "\n",
      "Decoded Text Output: <s>Let's try to tokenize!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80813fae-6b96-4932-94f7-73e7ae9ea27d",
   "metadata": {},
   "source": [
    "**Note: Given that now you understood how `AutoTokenizer` works, let's see an implementation to tokenize the input text efficiently.**\n",
    "\n",
    "The tokenizer returns a dictionary with three important items:\n",
    "\n",
    "- `input_ids` are the indices corresponding to each token in the sentence. The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
    "- `attention_mask` indicates whether a token should be attended to or not.\n",
    "- `token_type_ids` identifies which sequence a token belongs to when there is more than one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef00c7c4-f976-4495-8451-1a48c5d8f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# We can directly convert input text to tokens like follows\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = \"Let's try to tokenize!\"\n",
    "\n",
    "input_ids = tokenizer(inputs)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d4ecbf-2bd0-496f-8fb6-2c8463291dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9eca383-be81-4754-8e28-81b34a74cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88abf5-ec4b-416d-89ca-f7cc1a19baa5",
   "metadata": {},
   "source": [
    "**token_type_ids**  \n",
    "Some models‚Äô purpose is to do classification on pairs of sentences or question answering. These require two different sequences to be joined in a single ‚Äúinput_ids‚Äù entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:\n",
    "```python\n",
    "# [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n",
    "```\n",
    "\n",
    "We can use our tokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments (and not a list, like before) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832570bd-f7c4-41a5-b3f6-495cbb11c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eed443-523e-471c-b6f4-13132d663830",
   "metadata": {},
   "source": [
    "#### **Padding and Truncation**\n",
    "\n",
    "Reference: https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "\n",
    "Batched inputs are often different lengths, so they can‚Äôt be converted to fixed-size tensors. **Padding and truncation are strategies for** dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a **special padding token** to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences.\n",
    "\n",
    "In most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well. However, the API supports more strategies if you need them. The three arguments you need to are: `padding`, `truncation` and `max_length`.\n",
    "\n",
    "#### **Pad**\n",
    "\n",
    "Sentences aren‚Äôt always the same length which can be an issue because vectors/tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special *padding token* to shorter sentences.\n",
    "\n",
    "The padding argument controls padding. It can be a boolean or a string:\n",
    "\n",
    "- True or 'longest': pad to the longest sequence in the batch (no padding is applied if you only provide a single sequence).\n",
    "- 'max_length': pad to a length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). Padding will still be applied if you only provide a single sequence.\n",
    "- False or 'do_not_pad': no padding is applied. This is the default behavior.\n",
    "\n",
    "Set the padding parameter to True to pad the shorter sequences in the batch to match the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84eb6ba0-e487-40bf-8647-400f06eed126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 2021, 2054, 2055, 2117, 6350, 1029, 102, 0, 0, 0, 0, 0, 0],\n",
       " [101,\n",
       "  2123,\n",
       "  1005,\n",
       "  1056,\n",
       "  2228,\n",
       "  2002,\n",
       "  4282,\n",
       "  2055,\n",
       "  2117,\n",
       "  6350,\n",
       "  1010,\n",
       "  28315,\n",
       "  1012,\n",
       "  102],\n",
       " [101, 2054, 2055, 5408, 14625, 1029, 102, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "\n",
    "encoded_input = tokenizer(batch_sentences, padding=True)\n",
    "\n",
    "encoded_input[\"input_ids\"]\n",
    "\n",
    "# The first and third sentences are now padded with 0‚Äôs because they are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d6dd4b-e84f-4946-8f23-342639f7d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] don't think he knows about second breakfast, pip. [SEP]\n",
      "[CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for seq in encoded_input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95928b2c-cdc3-4a75-8548-a498d4af810f",
   "metadata": {},
   "source": [
    "#### **Truncate**\n",
    "\n",
    "On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you‚Äôll need to truncate the sequence to a shorter length.\n",
    "\n",
    "The truncation argument controls truncation. It can be a boolean or a string:\n",
    "\n",
    "- True or 'longest_first': truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached.\n",
    "- 'only_second': truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). This will only truncate the second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n",
    "- 'only_first': truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). This will only truncate the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n",
    "- False or 'do_not_truncate': no truncation is applied. This is the default behavior.\n",
    "\n",
    "Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e9c1578-cb72-4848-afa0-631b9eb5715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 2021, 2054, 2055, 2117, 6350, 102],\n",
       " [101, 2123, 1005, 1056, 2228, 2002, 102],\n",
       " [101, 2054, 2055, 5408, 14625, 1029, 102]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "\n",
    "encoded_input = tokenizer(batch_sentences, truncation=True, max_length=7)\n",
    "\n",
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a9acc1-39fa-4d39-b70b-4b743b4f8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] but what about second breakfast [SEP]\n",
      "[CLS] don't think he [SEP]\n",
      "[CLS] what about elevensies? [SEP]\n"
     ]
    }
   ],
   "source": [
    "for seq in encoded_input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18503bce-a46e-4a58-bcf0-c7cfccf6c4ee",
   "metadata": {},
   "source": [
    "#### **Build tensors**\n",
    "\n",
    "Finally, you want the tokenizer to return the actual tensors that get fed to the model.\n",
    "\n",
    "Set the `return_tensors` parameter to either **'pt' for PyTorch**, or **'tf' for TensorFlow**.  \n",
    "\n",
    "\n",
    "`return_tensors`: Acceptable values are:\n",
    "- 'tf': Return TensorFlow tf.constant objects.\n",
    "- 'pt': Return PyTorch torch.Tensor objects.\n",
    "- 'np': Return Numpy np.ndarray objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35bd6e8b-f0b4-4b74-a20b-3a000e360fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[  101,  2021,  2054,  2055,  2117,  6350,   102],\n",
       "       [  101,  2123,  1005,  1056,  2228,  2002,   102],\n",
       "       [  101,  2054,  2055,  5408, 14625,  1029,   102]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences, truncation=True, max_length=7, return_tensors=\"tf\")\n",
    "\n",
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f1cc2-1c71-4c96-9f1a-588d9df3ac3e",
   "metadata": {},
   "source": [
    "## **AutoModelFor`*`, TFAutoModelFor`*` and FlaxAutoModelFor`*`**\n",
    "\n",
    "We will show how to use those briefly, following this pattern:\n",
    "\n",
    "* Given input articles.\n",
    "* Tokenize them (converting to token indices).\n",
    "* Apply the model on the tokenized data to generate summaries (represented as token indices).\n",
    "* Decode the summaries into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c33c8401-c1cc-496d-9433-7ee95d652216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the pre-trained model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42225c5f-78b3-408b-bce0-7e54fd3cea8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: The full cost of damage in Newton S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: A fire alarm went off at the Holida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: Ferrari appeared in a position to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarize: John Edward Bates, formerly of Spal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarize: Patients and staff were evacuated f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summarize: Simone Favaro got the crucial try w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summarize: Veronica Vanessa Chango-Alverez, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summarize: Belgian cyclist Demoitie died after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summarize: Gundogan, 26, told BBC Sport he \"ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summarize: The crash happened about 07:20 GMT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             prompts\n",
       "0  summarize: The full cost of damage in Newton S...\n",
       "1  summarize: A fire alarm went off at the Holida...\n",
       "2  summarize: Ferrari appeared in a position to c...\n",
       "3  summarize: John Edward Bates, formerly of Spal...\n",
       "4  summarize: Patients and staff were evacuated f...\n",
       "5  summarize: Simone Favaro got the crucial try w...\n",
       "6  summarize: Veronica Vanessa Chango-Alverez, 31...\n",
       "7  summarize: Belgian cyclist Demoitie died after...\n",
       "8  summarize: Gundogan, 26, told BBC Sport he \"ca...\n",
       "9  summarize: The crash happened about 07:20 GMT ..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", \n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "pd.DataFrame(articles, columns=[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2a9e28b-74b2-486d-afa7-4502d3a1eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "tensor([[21603,    10,    37,  ...,     0,     0,     0],\n",
      "        [21603,    10,    71,  ...,     0,     0,     0],\n",
      "        [21603,    10, 21945,  ..., 18002,    21,     1],\n",
      "        ...,\n",
      "        [21603,    10, 21768,  ...,     0,     0,     0],\n",
      "        [21603,    10,  9982,  ...,     0,     0,     0],\n",
      "        [21603,    10,    37,  ...,     0,     0,     0]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "\n",
    "inputs = tokenizer(\n",
    "    articles, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    ")\n",
    "\n",
    "print(\"input_ids:\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\")\n",
    "print(inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8bd1c8f-f12a-43c5-8be7-a63cf04de00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     8,   423,   583,    13,  1783,    16, 20126, 16496,    19,\n",
      "           341,   271, 14841,     3,     5,   186,  7540,    16,   158,    15,\n",
      "          2296,     7,  5718,  2367, 14621,  4161,    57,  4125,   387,     3,\n",
      "             5,     3,     9,  8347,  5685,  3048,    16,   286,   640,     8],\n",
      "        [    0,  1472,  6196,   877,   326,    44,     8,  9108,    86,    29,\n",
      "            16,  6000,  1887,    30,  1856,     3,     5,  2554,   130,  1380,\n",
      "            12,  1175,     8,  1595,     3,     5,    80,    13,     8,   192,\n",
      "         14264,    19,    45, 13692,    63,     6,     8,   119,    45, 20576],\n",
      "        [    0,     3,   849,  2239,     7,   163, 14014,     3,    60,  8234,\n",
      "           232,   227,     3, 19585,   643,   845,   150,  8033,    47,   787,\n",
      "            30,   213,     3,    88,   225,  2447,     3,     5,     3,   849,\n",
      "          2239,     7,   497,     3,    31,    29,    32,   964,  8033,    47],\n",
      "        [    0,     8,     3,  3708,    18,  1201,    18,  1490,    19, 11970,\n",
      "            13,     3, 25345,     8, 10883,  2319,   344,  1332, 16583,    11,\n",
      "          1797,  9975,     3,     5,     3,    88,   177,   725,    66,     8,\n",
      "          3991,     6,   379,   192, 12052,    13,    16,   221,    75,  4392],\n",
      "        [    0,     3,     9,   388,  4281,  1058,    44,     8,  5998, 16026,\n",
      "            12,  4279,  2448,    11,   717,     3,     5,     3,     9,  1021,\n",
      "          2095,  5502,    47, 29766,    26,    45,     8,  2833,     3,     5,\n",
      "             8,  5415,   639, 18905,  7012,    16, 20958,   826,   633,  6032],\n",
      "        [    0,     3, 26705,  4463,     7,   989,  1891,     3,     9,  5695,\n",
      "            12,   579,  1840,     3,  3108,  2067,  1824,   400,  1823,    23,\n",
      "            63,  2551,  1967,    32,     3,     5,     8, 14580,     7,  1891,\n",
      "           166,  3511,    13,     8,   774,    12,     3,  3108,     3,     9],\n",
      "        [    0,     3, 31873, 30003, 24187,    32,    18,   188,    40,   624,\n",
      "           457,     6, 12074,    47,  4792,    11,   430,   388,  7532,    16,\n",
      "             8,  8420,     3,     5,  2095,   241,    12,  8320, 18050,  8688,\n",
      "             6, 14141,   113,    65,  2416,    12,     8,  9835,     3,     5],\n",
      "        [    0,     8,   944,    18,  1201,    18,  1490,    47,  1560,    57,\n",
      "             3,     9,  2340, 15214,   383,     8, 21689,    18,  1326,  4911,\n",
      "           397,    51,  1964,     3,     5,     8,  1964,  2804,   190,  8390,\n",
      "          2515,   663,     3,     5,     8,  2600,    31,     7,     3, 19585],\n",
      "        [    0,  4740, 10169,   152,   845,     3,    88,    54,   217,     8,\n",
      "          8619,   689,   227,     3, 30846,     3, 22052,   342,  6476, 27588,\n",
      "             7,    16,  1882,     3,     5,     8, 13692,  4785,     8,  1412,\n",
      "           296,  4119,   227,   223,  3730,    24,  2697,   376,    91,    21],\n",
      "        [    0,     8,  8420,  2817,    81, 10668,    10,  1755, 22866,    44,\n",
      "             8, 23704,    13,     8,    71, 22367,    11, 24583,  2409,    16,\n",
      "            90,  9031,    18,   106,    18,   134,    15,     9,     6, 25223,\n",
      "             3,     5,     8,   388,     6,  9742,    16,   112,   460,     7]])\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries\n",
    "\n",
    "summary_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                num_beams=2,\n",
    "                min_length=0,\n",
    "                max_length=40,\n",
    ")\n",
    "\n",
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b7423df-df8f-4396-95d9-6aba641a03be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoded_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the full cost of damage in Newton Stewart is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire alarm went off at the Holiday Inn in Hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stewards only handed reprimand after governing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 67-year-old is accused of committing the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a man receiving treatment at the clinic threat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregor Townsend gave a debut to powerhouse win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the 25-year-old was hit by a motorbike during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gundogan says he can see the finishing line af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the crash happened about 07:20 GMT at the junc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   decoded_summaries\n",
       "0  the full cost of damage in Newton Stewart is s...\n",
       "1  fire alarm went off at the Holiday Inn in Hope...\n",
       "2  stewards only handed reprimand after governing...\n",
       "3  the 67-year-old is accused of committing the o...\n",
       "4  a man receiving treatment at the clinic threat...\n",
       "5  Gregor Townsend gave a debut to powerhouse win...\n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...\n",
       "7  the 25-year-old was hit by a motorbike during ...\n",
       "8  gundogan says he can see the finishing line af...\n",
       "9  the crash happened about 07:20 GMT at the junc..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the generated summaries\n",
    "\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc83d17-cc85-4d92-aba9-2e249e021ad7",
   "metadata": {},
   "source": [
    "## **Fine-Tunning**\n",
    "\n",
    "https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266e326-1588-4286-b414-e32ab3335061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0dd13-753a-43e8-a241-b2248af084a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
